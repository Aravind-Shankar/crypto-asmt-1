Shaastra Sankalp ML Workshop

Image - 2D pixel array, rectangle. Pixel-value mapping, colorspace. RGB colorspace - amount of R,G,B in the pixel. Greyscale - lin combo of black and white. HSV - used while processing; same picture with different brightness.

Change in brightness scales all of RGB, but in HSV only V changes (H,S constant).

Usually we process only greyscale, because of computational speed and less memory compared to RGB/HSV.

Greyscale becomes white&black (binarized image a.k.a. mask) afterwards, based on some conditions as we need - e.g. a ball in the foreground with some background; ball white, bg black. This mask can be used to selectively show some part of an input image.

Convolution - given an image, we transform with a kernel matrix (sq. matrix with odd rows&cols). Weighted sum of superposition of kernel with image. Used for edge-detection/blurring etc. Corner elts can be convolved with extra padded zeros or can be truncated.

Smoothing images: removal of noise due to varied illuminations etc.
Averaging/block/window blur: K (kernel) = I(Identity,3x3) / 9.
Gaussian fn = exp(-x^2) and related fns. Smoothed upward cone in 2D. It is the limit of the binomial coeffs (1-2-1, 1-4-6-4-1 etc) as n->infinity.
Gaussian blur - blur with K taken from Gaussian fn. e.g. K=[[0,1,0],[1,2,1],[0,1,0]]/6. 1/6 = normalization factor, to ensure that values remain in 0-255 range (R/G/B channels or single greyscale channel).
Median blur - reduces salt-and-pepper noise. It is not convolution, but just fills as each elt the median of all adjacent elements (incl. itself)
All blurs make you lose features a.k.a edges (sharp changes in intensity).

Bilateral filter - removes noise, preserves edges. Sharp. Avoids merging of discrete lines, e.g. fringes in a diffraction pattern. Linear combo of blur (gaussian typically) + edge images of input.

Canny edge detection - gets edge images i.e. detects edges. First blur with appropriate kernel. Then convolve with Sobel-X and Sobel-Y separately to get partial derivatives. (Sobel-X: normalized [[-1,0,1],[-2,0,2],[-1,0,1]]) (Sobel-Y is Sobel-X transp.; they implement difference equations). The 2s and 1s are used for better approximation than just 1s and 0s. Now we have the gradient function, whose direction yields dirn of max change in intensity.
Non-maximal suppression - Move a small window (like a kernel) around the i/p image and find the maxima of Gx and Gy in the image; zero all the others, to leave only the possible edge pixels.
Canny's contribution - Keep 2 threshes minVal & maxVal; make all below minVal as zero, all above maxVal as 255; then whatever's left out is made 0 or 1 based on the adjacent pixels (more 0s means the value is set to 0, else 255).

Contours - collections (arrays) of equipotential (i.e. equal intensity) pixels. Clearly better, bigger contours obtained for quantized images (like black-and-white). Of immense functionality.
Hough transforms - curve detection. Transformation from (image)I-space to (Hough)H-space. I-space: Cartesian. H-space: polar (rho,theta) or (m,c) where we represent lines by y=mx+c.
y=mx+c representation: Take discrete values for m and c (both ranging from 0 to some max value). Make a zero matrix indexed by these values, called accumulator (rows: m, columns: c). Then for each m, iterate through the image and find c values. Increment the corres elt of the accumulator. Peak values of the accumulator entries occur at actual lines of the image. Similar reasoning for (rho,theta): plane polar coordinates. (rho = a.sin(theta) + b.cos(theta))

ML - SVM: take random init guess for fitting curve params. Give a form for the fitting fn (say a line) and also define the cost (deviation). Get the cost in terms of how wrong your guess is wrt all the test data given. Minimize wrt fitting curve variables (e.g. y=mx+c has m,c), called parameters. Things that vary in the test data (and which give the coordinates for fitting curve) are called features. Hyperplane linear approximation - a fn that is not approxable to a line in 2D may be approxed as one in more dimensions. Taylor-series like large power series expansion. Images are points in h.w.c-dimensional space (height h, width w, no. of channels c); in that space they are linearly separable, so hence we can teach a machine to learn the image of an 'A' when given many such images. Still linear separability is not guaranteed, hence we go for neural networks (that learns the non-linearity). There we can model a third function that is non-linear that can then be put to the SVM to get the curve right, but in general NN (neural networks) and its enhancement CNN can give much better results.

Coursera course on ML. ImageNet - huge data set of images